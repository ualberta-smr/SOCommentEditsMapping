# Can We Use Stack Overflow as a Source of Explainable Bug-fix Data?
This project analyzes crowd-sourced answers on Stack Overflow to determine a relationship between comments and edits on an answer. It does this by looping through the comments and edits on an answer and uses regular expressions to determine whether a comment caused an edit. It also keeps track of the time between comments and edits, as well as the authors of the comment, edit, question, and answer. The trends and statistics it observes are outputted in various CSVs, text files, and graphs.

# Instructions
The following is the set of instructions needed to run this project.

## Prerequisites
The program was written in Python 3.6.8. It requires sqlite3 V3.25 if using the source code as is. If using a lower sqlite3 version then there are comments in the source code that detail what to change.

This program assumes there is a copy of the SOTorrent dataset in a SQLite database named *sotorrent.sqlite3* located at the root of the project. The dataset used for this project is the SOTorrent version: Version 2019-09-23 linked [here](https://zenodo.org/record/3460115).

We used existing scripts from the [sotorrent-sqlite3 GitHub repo](https://github.com/awwong1/sotorrent-sqlite3) to create an SQLite database from the SOTorrent data, as the instructions provided with the SOTorrent dump create a MySQL database. This repo contains two scripts: 

* `get_and_verify_all.sh` which is used to download the *.xml.gz and *.csv.gz files of Version 2018-12-09 of the SOTorrent dump from [zenodo](https://zenodo.org/record/3460115) OR the user can download the version of SOTorrent they want manually (this project uses Version 2019-09-23). 

* `main.py` is what is used to create an sqlite database called *sotorrent.sqlite3* and populates it with the data in the SOTorrent dump. This process takes ~2 days.

This study requires these Python libraries to run the program properly:
* [Pandas](https://pandas.pydata.org/)
* [Matplotlib](https://matplotlib.org/)
* [Fuzzywuzzy](https://github.com/seatgeek/fuzzywuzzy)
* [NumPy](https://numpy.org/)

These can all be installed by running `pip3 install [library-name]`

## Running
The program can be run in the root directory of the project with the command `python3 src/main.py`

This program has multiple command line options that can be given:

`--clean` or `-c` has two values: `True` or `False`
* True is the **default** vale for this option. This option tells the program to **clean (delete) and create** the necessary SQL tables for the program to run.
* False will tell the program to assume the necessary tables are present and the SQL scripts in the *sql/* folder will not run.

`--type` or `-t` has two values: `Full` or `Stats`
* Full is the **default** value for this option. This option tells the program to run the full analysis. This means the program will retrieve the data needed from the dataset and generate the full results file (stored as **results.csv**), bubble plot of comment-edit distribution, and general statistics. The program will also generate the language specific result files (containing only matched comment-edit pairs), a statistics text file that is split by language tag, and a graph that shows the distribution of answer scores for each language.
* Stats tells the program to only generate the language specific result files, the statistics text file split by language tag, and the answer score distribution graphs. This will only work if a **results.csv** file (generated by a previous *Full* run) is present in the root directory.

`--user` or `-u` has two values: `True` or `False`
* True allows comments and edits to be matched by the same author. i.e., if a user makes a comment and then makes the edit themselves. 
* False is the **default** value for this option. We found that generally comments and edits with the same author that are matched do not tend to be useful. Therefore to reduce the amount of noise we set this option default False.

`--naive` or `-n` has two values: `True` or `False`
* True tells the program to naively pair comments and edits on a temporal basis alone. It matches a comment with the closest temporal edit. 
* False is the **default** value as comment-edit pair matching based on time is not accurate.

`--eval` or `-e` has two values: `True` or `False`
* True tells the program to evaluate itself against a given `ground_truth.csv`. If set to True the program will only run the evaluation and will not run the other parts of the program.
* False is the **default** value as we typically want to run the program instead of evaluating it.

## Regular expression patterns

The list of regular expression patterns we used is located in `src/regex_patterns.py` at the top of the file.

## SQL scripts

There are two sql scripts that are required by the program.

* `EditHistory.sql`

    This script is used to create the EditHistory table. This table aggregates the events(the initial body, body edits, and comments) of every question and answer with the event's creation date. This tables allows us to see a chronological history of each question and answer. 

* `EditHistory_Code.sql`

    This script creates the EditHistory_Code table. This table is similar to the EditHistory table. However, it does not store questions and question history, it only stores answers and answer history, and comments. The answers and answer histories also only store the code changes for each edit.

# Results
Our results of running the program on the five tags (java, javascript, php, Android, and Python) on SOTorrent version 2019-09-23 project are located [here](). This file also includes the database tables used.

The extracted zip folder and will contain multiple directories and files. These directories and files are described below.
To import the csvs into an sqlite3 database follow the steps in the **Importing** section

1. The `database_tables` directory contains the database tables needed for the program to run its analysis:

    * `QuestionIds.csv`
        
        This file contains the question ids of each of the five language tags used. 

    * `AnswerIds.csv`
        
        This file contains the answer ids of each of the five language tags used. They can be used to modify the queries in the code to isolate and focus on a specific language tag. e.g., `SELECT * FROM EditHistory_Code WHERE Event = 'InitialBody' AND PostId IN (SELECT Id from AnswerIds WHERE Tag = 'Java');`

    * `EditHistory_Code.csv`

        This table is what is used by the program to analyze comments and edits on answers. This table contains the answers, answer history, and comments that were analyzed by the program.

    * `EditHistory.csv`

        This table is the aggregation of all questions, and answers, and their histories and comments. This table is used predominantly for the creation of the EditHistory_Code table. However, the program does retrieve the questions (not the question history) to retrieve the question id.
    
Additionally the `PostBlockVersion.csv`, `PostHistory.csv`, `Posts.csv`, `PostVersion.csv`, and `Users.csv` files are provided if you wish to create the `EditHistory` and `EditHistory_Code` tables without wanting to download the entire SOTorrent dump.

2. The `results` directory contains the results of running the program on each language tag separately:

    * `<tag>_results.csv`

        These five files are the complete results of running the program on the individual language tags. There are many rows in the csvs and to view the entire results will most likely require the importing of the csvs into a database table. 
       
    * `<tag>_stats.txt`
    
        These five files are some descriptive statistics for each tag.

3. The `ground_truth` directory contains the files used to determine the ground truth in the *Extracting Comment-Edit Pairs* section of the paper: 

    * `<tag>.csv`

        These five csv files are the results of the authors ground truth analysis of comments for 20 questions in each of the focused language tags. The *Resolution* column are the edit ids agreed upon by the authors. This table is used in the *Comparison with Ground Truth* subsection or the *Mapping Comments to Edits* section of the paper

4. The `general_precision` directory contains files used in answering the 4 research questions in sections *Precision of Comment-Edit Pairs*, *Tangled Changes*, *Types of Changes in Comment-Edit Pairs*, and *Usefulness of Comment-Edit Pairs* of the paper:

    * `<tag>.csv`
    
        These five files are the raw analysis of both authors. Each file contains the 382 randomly sampled pairs for that language and details the initial analysis of each author as well as the resolutions of any disagreements. The analysis includes the initial categories assigned, the usefulness, and tangledness of the comment-edit pairs.

    * `kappa_stats_by_tag.csv` 

        This file details the calculations for the Cohen's Kappa coefficient for each tag. This is used to create tables 4 and 5 of the paper.
        
    * `kappa_stats_by_category.csv` 

        This file details the calculations for the Cohen's Kappa coefficient for each category. This was used for section `RQ3: Types of Changes in Comment-Edit Pairs`

    * `stats.csv` 

        This file contains the statistics of each tag and language. It describes the number of confirmed comment-edit pairs for each category for each type, as well as their usefulness and tangledness. This is used to create table 7 of the paper.
        
    * `categories.csv` 

        This file details the categories used in typing each comment-edit pair. It contains the archetypes of comments we find and how they fit into the relevant [TSE](https://petertsehsun.github.io/papers/so_comment_empirical_tse2020.pdf) categories. Some notes are provided on how we determined whether a comment fit into a category or not.
        
5. The `pull_requests.csv` file contains the details of the 15 comment-edit pairs used to make pull requests on open source repositories. The file details which part of the edit was used as well as how the comment was paraphrased (if it was) on the pull request. The links to the repositories and pull requests have also been provided.

## Importing

To import csvs into an sqlite3 database follow these steps:

1. Start sqlite3 with an empty database using `sqlite3 <database_name>.sqlite3`

2. Create tables to store the data in the csvs. You can write a *.sql file and run it in sqlite using `.read path-to-script`
    * The EditHistory and EditHistory_Code schemas are already provided in the `sql/EditHistory.sql` and `sql/EditHistory_Code.sql` files

3. Change the read mode of sqlite3 by using `.mode csv`

4. Import the csvs by runnning `.import "path-to-csv" <table-name>`

# License
MIT License

Copyright (c) 2020 UAlberta Software Maintenance and Reuse (SMR) research group

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
